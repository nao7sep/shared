## brainstorming

i wrote half of brainstorming part of what.md. i gave it to claude code (sonnet 4.5), conversed about it until it said everything was clear, while adding seemingly important responses of mine to the file.

this is my first time vibe coding. going through about 5 iterations of ai chat, i added some paragraphs without separators. only then i started adding separators. the first big part is not one part; it is the first 5 iterations combined.

## implementation doc and code

i had how.md generated by the ai. i asked whether i should read it precisely or have at least a quick look or not read it at all. ai suggested not to read it because testing code would find design flaws and bugs faster than my reading how.md. i agreed, cleared the conversation log and had the code generated.

just one instruction i gave additionally before having the code generated was to change the folder structure. ai suggested tk/tk. i changed it to tk/src/tk, which should be more consistent with python guidelines.

## text-based files

i checked text-based files such as pyproject.toml and readme.md. i intended to fix a few things and only ended up fixing my misconception. next time, i probably dont have to check text-based files. i should go straight to testing the app.

## running tests

it should be beneficial for myself to note that i asked the ai whether i should proceed to reviewing the code or run the app to test it. as a c#er, reviewing the code or at least taking a quick look at vital parts seemed important. models, profile management, kvp keys, etc often reveal a lot of things. but ai, as anticipated, said i should just run the app to find problems faster.

i believe this is also a python thing. in c#, a precompiled language, if code compiles, there's no syntax error. in python, syntax errors may come out only when app gets there.

as i ran the app and played with it, a lot of inconsistencies between my thoughts and the ai's code were detected. i iteratively worked on them.

## code review

once i started feeling a sense of stability in the app and there were no more obvious inconsistencies, i took a quick look at the code. the code was "kind of" separated into concerns, but the separation was far from ideal. like extracting user timezone is definitely not profile.py's responsibility. but when in rome...

funny thing is that there wasnt much to adjust in code. so, ai was correct. i should run the app and just complain. while test-running the app and fixing problems, i was reading code diffs. so i cant say i didnt read any code at all before tests finished. but i think it was good experience to debug an app without reading much code.

## ai code reviews

until here it was claude code with sonnet 4.5 all the way.

then i used vsc's copilot chat (sonnet 4.5 auto selected), gemini cli (probably gemini 3 pro due to the complexity of the task), codex (perhaps gpt 5.2 codex; didnt really check) to review the code. i stated i had no intention in micro optimization and asked for actual problems. sonnet found many. gemini, 5 or so. codex, 3, i think. i asked them to save reports. then i cleared the conversation log of claude code (still sonnet 4.5), asked it to read the 3 review reports and review everything in the package.

claude code fixed 10+ problems. i asked the other 3 agents to re-review the code. copilot chat (sonnet 4.5) said everything was solved. gemini said a few issues were remaining. codex too. i had these 2 agents' reports saved and repeated the same thing. then gemini and codex too said everything was fixed.

then i made sure the diffs were insignificant. no large refactoring or "patches over patches" were observed.

## NOT updating how.md

this is not a log of work, rather a decision note: i will not have how.md updated. its sole purpose was to save all vital points of the initial conversation. i had what.md, but it was messy. along the way of initial design, i asked some questions and ai clarified things, which i then agreed on. so, what.md alone wouldnt have been enough for implementation.

i actually thought about deleting what.md too once i had how.md, but ai suggested me to keep it. then i added more and more instructions to it. i am not sure how to define the actual purpose of what.md now and part of me wants to delete it together with how.md, but deletion can be done any time. keeping the files currently doesnt hurt. if ais get confused or something because of them, eventually, i'll find a new place for them.

## bug fixes and improvements

i fixed bugs in one iteration and made improvements in the following one.

major discomfort with python comes from differences between python culture and c# culture. so, i still ask ai to read transition-guide before any work. i might make a more concise version of this to save context, but not today. i'll work on typescript, node.js, etc as well. when i know enough about what i am playing with, i'll try reducing this 48 kb set of documents to 5-10 kb, while adding ts-related info.

as i added __TEST_API_KEYS.json (which is .gitignored), i also renamed WHAT.md, HOW.md and WORK_LOG.md by prepending _ as prefix.

part of me still wants to delete them or move them to a private repository, but they, especially WHAT.md, contain a lot of refined-through-chat specs as plaintext and i probably can ask ais to refer to them to obtain pure WHAT rather than WHAT (specs) and HOW (implementation) tangled in code. one issue of the latter is ais may mimic the implementation as well when all i want is that they understand what i want them to do and think of optimal NEW implementation for the circumstances.

then, if i want to give a prompt like "please refer to tk's _WHAT.md and understand how it has been done before" for another open source tool development, tk's _WHAT.md cant be in a private repo. if one is public, all must be public. if not, other developers (and future i) wont be able to analyze what i was initially thinking. 
