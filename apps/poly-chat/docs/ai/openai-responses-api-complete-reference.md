# OpenAI Responses API: Complete Technical Reference and Implementation Guide

## 1. Executive Summary

The OpenAI Responses API represents a fundamental architectural evolution in programmatic interaction with Large Language Models. Moving beyond the stateless "text-in, text-out" paradigm of the Chat Completions API, the Responses API introduces an "agentic-first" architecture designed to support the next generation of reasoning models (o3, GPT-5) and facilitate complex, multi-turn agentic workflows natively on the server.

The Responses API consolidates text generation, tool calling, web searching, file referencing, and computer use into a unified "agentic loop." It offers built-in state management via Conversation objects and Context Compaction, reducing the complexity of client-side history management while enabling sophisticated multi-turn interactions.

This document provides a comprehensive technical reference covering:

- Basic text generation and prompt engineering
- Multi-turn conversation management patterns
- Advanced state management with server-side conversations
- Reasoning model configuration
- Reusable prompt templates
- File uploads and multimodal inputs
- Web search configuration
- Error handling and resilience patterns
- Streaming implementations
- Zero Data Retention architectures

## 2. Getting Started with Text Generation

### 2.1 Basic Text Generation

The simplest use case for the Responses API is generating text from a single prompt. The API accepts a model identifier and an `input` parameter containing your prompt.

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5.2",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)
```

### 2.2 Understanding the Output Structure

The response contains an `output` array with content generated by the model. Each item in the array represents a distinct piece of outputâ€”text messages, tool calls, reasoning tokens, or media.

A basic text response structure looks like:

```json
{
  "output": [
    {
      "id": "msg_67b73f697ba4819183a15cc17d011509",
      "type": "message",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
          "annotations": []
        }
      ]
    }
  ]
}
```

**Important**: The `output` array frequently contains more than one item. It can include tool calls, reasoning data, and other content types. Never assume that text output is located at `output[0].content[0].text`.

The Python SDK provides a convenience property `response.output_text` that aggregates all text outputs into a single string. While convenient for simple cases, robust applications should inspect the full `output` array to handle tool invocations and annotations properly.

## 3. Architectural Evolution: From Chat Completions to Responses

### 3.1 The Shift from Messages to Input Items

The most immediate distinction for developers migrating from `client.chat.completions.create` to `client.responses.create` is the replacement of the `messages` array with a polymorphic `input` parameter.

In the Chat Completions API, context was constructed using message objects with strict role typing (`system`, `user`, `assistant`). The Responses API introduces **Input Items**â€”a flexible collection that can contain text messages, images, audio clips, references to previous responses, or tool outputs.

#### Table 1: API Comparison

| Feature | Chat Completions API | Responses API |
|---------|---------------------|---------------|
| Primary Method | `client.chat.completions.create` | `client.responses.create` |
| Context Container | `messages=` (List of Message objects) | `input=` (List of Input Items) |
| System Instruction | Role: `system` | Role: `developer` or `instructions` parameter |
| Parallel Generation | `n` parameter (returns choices array) | Removed. Single generation per request. |
| State Management | Client-side only | Native `conversation_id`, `previous_response_id` |
| Output Access | `choices[0].message.content` | `output` array or `output_text` convenience |
| Tool Execution | Client-side loop required | Server-side agentic loop supported |

### 3.2 Message Roles and the Instruction Hierarchy

The Responses API formalizes a chain of command for instructions using the **`instructions` parameter** and message roles. According to the [OpenAI Model Spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command), models prioritize instructions in the following order:

1. **`instructions` parameter**: Highest priority system-level directives
2. **`developer` role messages**: Application-level business logic and guardrails
3. **`user` role messages**: End-user input and queries
4. **`assistant` role messages**: Previous model responses

#### Table 2: Message Role Hierarchy

| Role | Purpose | Priority Level |
|------|---------|----------------|
| `developer` | Instructions from the application developer, containing business logic and safety constraints | High (below instructions parameter) |
| `user` | Instructions from the end user | Medium |
| `assistant` | Messages generated by the model in previous turns | N/A (historical context) |

### 3.3 Using the Instructions Parameter

The `instructions` parameter provides the most authoritative way to specify system behavior. Instructions defined here take precedence over all input messages.

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "low"},
    instructions="Talk like a pirate.",
    input="Are semicolons optional in JavaScript?"
)

print(response.output_text)
# Output uses pirate speech patterns
```

**Note**: The `instructions` parameter only applies to the current request. When using `previous_response_id` for conversation chaining, instructions from previous turns are not automatically carried forward. You must re-specify them in each request where they should apply.

### 3.4 Using Developer Role Messages

Alternatively, you can embed instructions within the `input` array using the `developer` role:

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "low"},
    input=[
        {
            "role": "developer",
            "content": "Talk like a pirate."
        },
        {
            "role": "user",
            "content": "Are semicolons optional in JavaScript?"
        }
    ]
)
```

This pattern is functionally similar to using the `instructions` parameter but allows you to include developer directives within the conversation history. This is particularly useful when managing multi-turn conversations where you want system-level constraints to be part of the persistent context.

### 3.5 Understanding Output Polymorphism

Unlike the Chat Completions API which reliably returned text in `choices[0].message.content`, the Responses API returns a generic `output` array that may contain:

- **Text content**: The model's spoken response
- **Tool calls**: Requests to execute web searches, file lookups, or custom functions
- **Reasoning tokens**: For reasoning models like o3, the internal chain of thought (often encrypted or summarized)
- **Audio/Image**: Generated media assets

Developers must inspect the `output` list to correctly handle all content types, especially when tool calls are involved.

## 4. Managing Multi-Turn Conversations

One of the most common migration challenges is understanding how to send a full conversation history to the Responses API, similar to how the Chat Completions API accepted an array of messages. The Responses API supports this pattern directly through the `input` array.

### 4.1 Passing Full Conversation History

To build a multi-turn conversation, you pass the complete message history in the `input` array, just as you did with the Chat Completions API's `messages` parameter.

```python
from openai import OpenAI
client = OpenAI()

# Turn 1: Initial question
conversation_history = [
    {"role": "user", "content": "What is the capital of France?"}
]

response1 = client.responses.create(
    model="gpt-5",
    input=conversation_history
)

# Append assistant's response to history
conversation_history.append({
    "role": "assistant",
    "content": response1.output_text
})

# Turn 2: Follow-up question
conversation_history.append({
    "role": "user",
    "content": "What's the population of that city?"
})

response2 = client.responses.create(
    model="gpt-5",
    input=conversation_history
)

print(response2.output_text)
```

### 4.2 Including System Instructions in Multi-Turn Conversations

When managing multi-turn conversations, you can include `developer` role messages at the beginning of the history to establish system-level behavior:

```python
from openai import OpenAI
client = OpenAI()

# Initialize conversation with system instructions
conversation_history = [
    {
        "role": "developer",
        "content": "You are a helpful assistant specialized in French history and culture. Always provide citations when giving historical facts."
    }
]

# User's first message
conversation_history.append({
    "role": "user",
    "content": "Tell me about the French Revolution."
})

response1 = client.responses.create(
    model="gpt-5",
    input=conversation_history
)

# Add response to history
conversation_history.append({
    "role": "assistant",
    "content": response1.output_text
})

# Continue the conversation
conversation_history.append({
    "role": "user",
    "content": "What role did Napoleon play in its aftermath?"
})

response2 = client.responses.create(
    model="gpt-5",
    input=conversation_history
)
```

### 4.3 Handling Complex Output Types in Conversation History

When the model's output includes tool calls, reasoning tokens, or other non-text content, you need to properly represent these in the conversation history. The full output structure should be preserved:

```python
# If the previous response included tool calls or other content types
previous_output = response1.output  # Get the full output array

for item in previous_output:
    if item.type == "message":
        # Add text messages to history
        conversation_history.append({
            "role": "assistant",
            "content": item.content[0].text
        })
    elif item.type == "web_search_call":
        # Tool calls should also be preserved in history
        # This ensures the model knows what searches were performed
        conversation_history.append({
            "role": "assistant",
            "content": f"[Performed web search: {item.action.query}]"
        })
```

### 4.4 Chat Application Pattern: Complete Example

Here's a complete pattern for a chat application that maintains conversation history:

```python
from openai import OpenAI

class ChatSession:
    def __init__(self, system_prompt: str = None):
        self.client = OpenAI()
        self.history = []

        # Initialize with developer instructions if provided
        if system_prompt:
            self.history.append({
                "role": "developer",
                "content": system_prompt
            })

    def send_message(self, user_message: str) -> str:
        # Add user message to history
        self.history.append({
            "role": "user",
            "content": user_message
        })

        # Generate response with full history
        response = self.client.responses.create(
            model="gpt-5",
            input=self.history
        )

        # Extract text response
        assistant_message = response.output_text

        # Add assistant response to history
        self.history.append({
            "role": "assistant",
            "content": assistant_message
        })

        return assistant_message

    def get_history(self):
        return self.history


# Usage
chat = ChatSession(
    system_prompt="You are a helpful coding assistant. Provide concise, accurate answers with code examples when appropriate."
)

response1 = chat.send_message("How do I read a file in Python?")
print(response1)

response2 = chat.send_message("How do I handle errors when the file doesn't exist?")
print(response2)

# The full conversation context is maintained automatically
print(f"\nFull conversation has {len(chat.get_history())} messages")
```

### 4.5 Token Management for Long Conversations

As conversations grow, you'll eventually hit context limits. You have several options:

**Option 1: Manual History Truncation**
```python
# Keep only the last N messages plus the system prompt
MAX_HISTORY = 20

def trim_history(history):
    # Always keep the first message if it's a developer instruction
    system_messages = [msg for msg in history if msg["role"] == "developer"]
    conversation_messages = [msg for msg in history if msg["role"] != "developer"]

    # Keep only recent conversation messages
    recent_messages = conversation_messages[-MAX_HISTORY:]

    return system_messages + recent_messages
```

**Option 2: Use Server-Side State Management** (covered in Section 5)
- Use `previous_response_id` for automatic chaining
- Use `conversation_id` for persistent server-side storage
- Use Context Compaction to compress history

## 5. Reasoning Models and Effort Configuration

The Responses API introduces explicit control over reasoning model behavior through the `reasoning` parameter. This is critical for models like GPT-5 and o3 that perform extended chain-of-thought processing.

### 5.1 The Reasoning Parameter

The `reasoning` parameter accepts an `effort` field that controls how much computational reasoning the model performs:

- **`"low"`**: Minimal reasoning, faster responses
- **`"medium"`**: Balanced reasoning (default for most models)
- **`"high"`**: Maximum reasoning depth, best quality but slower

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "high"},
    input="Explain the proof of Fermat's Last Theorem in detail."
)
```

### 5.2 When to Use Different Effort Levels

**Use `effort: "low"` for:**
- Simple factual questions
- Formatting or transformation tasks
- High-throughput scenarios where speed matters

**Use `effort: "medium"` for:**
- Standard conversational interactions
- Moderate complexity questions
- Balanced cost/quality scenarios

**Use `effort: "high"` for:**
- Complex mathematical proofs
- Deep analytical tasks
- Critical decisions where quality trumps speed
- Research and analysis tasks

### 5.3 Combining Reasoning with Instructions

The `reasoning` parameter works seamlessly with the `instructions` parameter:

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "high"},
    instructions="Analyze this code for security vulnerabilities. Be thorough and consider edge cases.",
    input="Review this authentication function: [code]"
)
```

### 5.4 Important Performance Considerations

Reasoning models with high effort settings can take significantly longer to respond. See Section 9 on timeout configuration and Section 10 on streaming to handle this appropriately.

## 6. Advanced State Management Architectures

While passing full conversation history in the `input` array is straightforward and familiar, the Responses API offers sophisticated server-side state management options that reduce bandwidth, improve performance, and enable advanced features.

### 6.1 Explicit Chaining via previous_response_id

For developers who want to maintain some control over state while reducing payload size, the `previous_response_id` parameter offers a lightweight chaining mechanism.

```python
# Turn 1: Initial Interaction
resp1 = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Define the concept of 'Recursion' in computer science."}]
)

# Turn 2: Chained Interaction
resp2 = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Now write a Python function demonstrating it."}],
    previous_response_id=resp1.id  # Implicitly pulls context from resp1
)
```

This method creates a linked list of interactions without re-uploading the full history. However, it is mutually exclusive with the `conversation` parameter and is best for short-lived, linear sessions.

**Important**: When using `previous_response_id`, the `instructions` parameter from the previous request is **not** automatically carried forward. You must re-specify system instructions in each request.

### 6.2 Server-Side Conversation Objects

For persistent, long-running applications (customer support bots, personal assistants), the `conversation` parameter shifts state management entirely to OpenAI's infrastructure.

```python
# Create a new conversation
conversation_id = "conv_abc123"  # Or let OpenAI generate one

# Turn 1
response1 = client.responses.create(
    model="gpt-5",
    conversation=conversation_id,
    input=[{"role": "user", "content": "Hello, I need help with Python."}]
)

# Turn 2 - history is automatically maintained server-side
response2 = client.responses.create(
    model="gpt-5",
    conversation=conversation_id,
    input=[{"role": "user", "content": "How do I read files?"}]
)
```

When a `conversation_id` is provided:
- **Context Injection**: Existing conversation history is automatically prepended to the current `input`
- **Automatic Persistence**: The current input and output are appended to the conversation upon completion
- **Reduced Bandwidth**: Only the delta of new information is transmitted

### 6.3 Context Compaction

As conversations grow, they approach the model's context limit. **Context Compaction** addresses this by compressing prior messages into a single "Compaction Item" that retains semantic meaning while occupying fewer tokens.

#### 6.3.1 Automatic Compaction

Configure automatic compaction rules via the `context_management` parameter:

```python
response = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Analyze the following legal precedents..."}],
    context_management={
        "type": "automatic",
        "max_tokens": 100000,
        "compaction_threshold": 0.8  # Compact when 80% full
    }
)
```

When the threshold is breached, the API automatically compacts history before processing the new input.

#### 6.3.2 Manual Compaction for Zero Data Retention

For Zero Data Retention (ZDR) architectures where storing conversation history on OpenAI servers is prohibited:

```python
# Manual Compaction Flow
compact_response = client.responses.compact(
    input=full_conversation_history,
    model="gpt-5"
)
compaction_item = compact_response.output  # Contains encrypted_content

# Next Request
response = client.responses.create(
    model="gpt-5",
    input=[compaction_item, {"role": "user", "content": "Continue..."}]
)
```

This enables "Stateless Statefulness"â€”the client holds encrypted state locally, and the server decrypts it only during inference.

## 7. Reusable Prompts and Templates

The Responses API supports dashboard-managed reusable prompts that decouple prompt engineering from application code. This enables non-technical stakeholders to iterate on prompts without code deployments.

### 7.1 Creating Reusable Prompts

In the [OpenAI dashboard](https://platform.openai.com/chat/edit), you can create prompt templates with placeholders like `{{customer_name}}` and `{{product}}`.

### 7.2 Using Reusable Prompts in API Requests

Reference dashboard prompts using the `prompt` parameter:

```python
response = client.responses.create(
    model="gpt-5",
    prompt={
        "id": "pmpt_abc123",
        "version": "2",
        "variables": {
            "customer_name": "Jane Doe",
            "product": "40oz juice box"
        }
    }
)

print(response.output_text)
```

The `prompt` parameter has three properties:
- **`id`**: Unique identifier of your prompt (found in dashboard)
- **`version`**: Specific version string (defaults to "current")
- **`variables`**: Map of values to substitute for placeholders

### 7.3 Benefits of Reusable Prompts

**Decoupling**: Prompt engineers can iterate independently of developers
**Versioning**: Track prompt changes and roll back if needed
**A/B Testing**: Compare prompt versions without code changes
**Governance**: Centralized management of sensitive instructions

### 7.4 Advanced Variable Types

Variables can be strings, images, or files:

```python
# String variables (simple)
variables = {
    "topic": "Dragons",
    "tone": "educational"
}

# File variables (multimodal)
variables = {
    "topic": "Dragons",
    "reference_pdf": {
        "type": "input_file",
        "file_id": "file-abc123"
    }
}
```

## 8. File Uploads and Multimodal Inputs

The Responses API supports file uploads for multimodal prompting. Files are uploaded to OpenAI's storage and referenced by ID in subsequent requests.

### 8.1 Uploading Files

```python
import openai

client = openai.OpenAI()

# Upload a PDF
file = client.files.create(
    file=open("company_policies.pdf", "rb"),
    purpose="user_data"
)

print(f"File uploaded: {file.id}")
```

### 8.2 Using Files in Prompts

Once uploaded, reference files in the `input` array using the `input_file` type:

```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {
            "role": "developer",
            "content": "You are an expert policy analyst."
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "input_file",
                    "file_id": file.id
                },
                {
                    "type": "text",
                    "text": "Summarize the vacation policy from this document."
                }
            ]
        }
    ]
)

print(response.output_text)
```

### 8.3 Using Files in Reusable Prompt Variables

Files can also be passed as variables to reusable prompts:

```python
# Upload a PDF
file = client.files.create(
    file=open("draconomicon.pdf", "rb"),
    purpose="user_data"
)

# Use in prompt template
response = client.responses.create(
    model="gpt-5",
    prompt={
        "id": "pmpt_abc123",
        "variables": {
            "topic": "Dragons",
            "reference_pdf": {
                "type": "input_file",
                "file_id": file.id
            }
        }
    }
)

print(response.output_text)
```

### 8.4 Supported File Types

The API supports various file formats including:
- PDFs (`.pdf`)
- Images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`)
- Text files (`.txt`, `.md`, `.csv`)
- Documents (`.docx`)

Refer to the [official documentation](https://platform.openai.com/docs/api-reference/files) for the complete list and size limits.

## 9. Web Search Tool Configuration and Agentic Workflows

The Responses API elevates Web Search from an external plugin to a native capability, allowing models to autonomously query the internet to ground responses in real-time data.

### 9.1 Configuring Web Search

Enable web search by including it in the `tools` array:

```python
response = client.responses.create(
    model="gpt-5",
    tools=[{
        "type": "web_search",
        "filters": {
            "allowed_domains": ["nature.com", "arxiv.org", "nasa.gov"]
        },
        "user_location": {
            "type": "approximate",
            "country": "US",
            "city": "Boston",
            "timezone": "America/New_York"
        }
    }],
    input=[{"role": "user", "content": "What are the latest breakthroughs in exoplanet atmospheric analysis?"}]
)
```

### 9.2 Domain Filtering for Reliability

**Domain Filtering** restricts the model's searches to trusted sources:

- **Mechanism**: The `filters.allowed_domains` list
- **Capacity**: Up to 100 distinct domains
- **Syntax**: Omit protocols (`https://`). Subdomains are automatically included (e.g., `nasa.gov` includes `jwst.nasa.gov`)
- **Availability**: Exclusive to Responses API

This prevents hallucinations from low-quality sources and ensures compliance in regulated industries.

### 9.3 Location-Aware Search

The `user_location` parameter grounds searches in a specific geographic context:

```python
"user_location": {
    "type": "approximate",
    "country": "US",        # ISO-3166-1 alpha-2 code
    "city": "Boston",       # Free-text string
    "region": "MA",         # Free-text string
    "timezone": "America/New_York"  # IANA timezone identifier
}
```

**Limitation**: Not supported for "Deep Research" tasks.

### 9.4 Citations and Source Transparency

The API provides two levels of transparency:

**Inline Citations**: Annotations linking specific text to URLs

**Full Sources List**: Complete audit trail of all pages consulted

```python
response = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Summarize the market reaction to the merger."}],
    tools=[{"type": "web_search"}],
    include=["web_search_call.action.sources"]  # Request full audit trail
)

# Access full source list
for item in response.output:
    if item.type == "web_search_call":
        print(f"Consulted {len(item.action.sources)} sources:")
        for source in item.action.sources:
            print(f"  - {source.url}")
```

This reveals every URL processed, including real-time feeds like `oai-finance` or `oai-weather`.

## 10. Resilience Engineering: Error Handling, Timeouts, and Retries

Reasoning models and agentic loops introduce new latency profiles. Simple completions take milliseconds, but "Deep Research" tasks can take minutes. This requires careful timeout and error handling.

### 10.1 Granular Timeout Configuration

The default SDK timeout is 10 minutes (600 seconds). For production, configure granular timeouts using `httpx.Timeout`:

```python
import httpx
from openai import OpenAI

timeout_config = httpx.Timeout(
    connect=5.0,    # 5s to establish TCP connection
    read=120.0,     # 120s to wait for server response
    write=10.0,     # 10s to send payload
    pool=10.0       # 10s to wait for connection pool
)

client = OpenAI(timeout=timeout_config)
```

**The "Thinking" Trap**: Reasoning models like o3 perform "Chain of Thought" processing before emitting the first token. If the read timeout is shorter than the thinking time, the client will raise `APITimeoutError` prematurely. For reasoning models, use significantly higher read timeouts (60s+) or enable streaming.

### 10.2 Retry Logic and Exponential Backoff

The SDK includes default retry logic (2 retries) for transient errors:
- Connection errors
- Timeouts (408)
- Rate Limits (429)
- Server Errors (5xx)

**Override retry behavior per-request:**

```python
response = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Analyze this data..."}],
    max_retries=5  # Increase retries for critical calls
)
```

For 429 (Rate Limit) errors, the `x-ratelimit-reset-tokens` or `retry-after` headers provide exact wait times. Consider using `tenacity` for advanced retry strategies with jitter.

### 10.3 Handling Specific Exceptions

Catch specific exceptions from the `openai` library hierarchy:

#### Table 3: Error Handling Matrix

| Exception Type | HTTP Code | Root Cause | Recommended Strategy |
|----------------|-----------|------------|----------------------|
| `APIConnectionError` | N/A | Network/DNS failure | Retry with backoff. Check firewall/proxy settings. |
| `APITimeoutError` | 408 | Request exceeded read timeout | Do not retry immediately. Switch to Streaming or increase timeout. |
| `RateLimitError` | 429 | Quota or RPM limit hit | Retry with exponential backoff & jitter. |
| `BadRequestError` | 400 | Invalid JSON / Missing ID | Never retry. Code fix required. Often caused by missing tool_call history. |
| `APIStatusError` | 5xx | OpenAI Server issue | Retry. Check OpenAI Status page. |

**Critical Pattern**: A specific 400 error, `No tool call found for function call output`, occurs when tool outputs are submitted without the preceding `tool_call` in the conversation history. Always maintain complete conversation state when using tools.

```python
from openai import OpenAI, APITimeoutError, RateLimitError, BadRequestError
import time

def resilient_request(client, **kwargs):
    max_attempts = 3
    base_delay = 1.0

    for attempt in range(max_attempts):
        try:
            return client.responses.create(**kwargs)

        except APITimeoutError:
            # Don't retry timeouts immediately - might be a long-running task
            raise

        except RateLimitError as e:
            if attempt < max_attempts - 1:
                # Exponential backoff with jitter
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(delay)
            else:
                raise

        except BadRequestError:
            # Never retry bad requests - fix the code
            raise

        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(base_delay * (2 ** attempt))
            else:
                raise
```

## 11. Streaming and Real-Time Event Processing

For reasoning models and long-running tasks, streaming is essential for user experience. It allows users to see progress rather than watching a spinner for 60+ seconds.

### 11.1 The Event Lifecycle

The Responses API uses Server-Sent Events (SSE). When `stream=True`, the method returns an iterable of event objects:

- **`response.created`**: Emitted once at initialization
- **`response.output_item.added`**: New item (message, tool call) started
- **`response.output_text.delta`**: Text content chunks
- **`response.function_call_arguments.delta`**: JSON chunks for tool calls
- **`response.output_item.done`**: Item completed
- **`response.completed`**: Final event with token usage statistics

### 11.2 Implementing Stream Consumption

Correctly parsing events requires a state machine approach:

```python
response_stream = client.responses.create(
    model="gpt-5",
    input=[{"role": "user", "content": "Research the history of the transistor."}],
    stream=True
)

print("Assistant: ", end="")
for event in response_stream:
    # Handle Text
    if event.type == "response.output_text.delta":
        print(event.delta, end="", flush=True)

    # Handle Tool Call Start
    elif event.type == "response.output_item.added":
        item = event.item
        if item.type == "web_search_call":
            print(f"\n[Searching web...]")

    # Handle Completion & Usage
    elif event.type == "response.completed":
        usage = event.response.usage
        print(f"\n\n[Used {usage.total_tokens} tokens]")
```

### 11.3 Building Status Indicators

Streaming enables sophisticated UX patterns like skeleton screens:

```python
status_messages = {
    "web_search_call": "ðŸ” Searching the web...",
    "function_call": "ðŸ”§ Executing tool...",
    "message": "ðŸ’­ Thinking..."
}

for event in response_stream:
    if event.type == "response.output_item.added":
        status = status_messages.get(event.item.type, "Processing...")
        print(f"\n{status}")
    elif event.type == "response.output_text.delta":
        print(event.delta, end="", flush=True)
```

This dramatically improves perceived latency.

## 12. Security and Compliance: Zero Data Retention (ZDR)

The Responses API supports privacy-preserving architectures for industries with strict data retention policies (HIPAA, SOC2).

### 12.1 The Encrypted Reasoning State

By adding `include=["reasoning.encrypted_content"]` to the request, the API returns the model's internal reasoning tokens as an encrypted blob:

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "high"},
    input=[{"role": "user", "content": "Analyze this medical record..."}],
    include=["reasoning.encrypted_content"]
)

# Extract encrypted reasoning state
encrypted_state = None
for item in response.output:
    if hasattr(item, 'reasoning') and hasattr(item.reasoning, 'encrypted_content'):
        encrypted_state = item.reasoning.encrypted_content
        break

# Store encrypted_state locally on your secure infrastructure
```

### 12.2 Passing Encrypted State to Subsequent Requests

In the next turn, pass the encrypted state back to the model:

```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {
            "type": "reasoning_state",
            "encrypted_content": encrypted_state
        },
        {
            "role": "user",
            "content": "Continue the analysis..."
        }
    ]
)
```

This allows the model to "remember" its deep reasoning from the previous turn without OpenAI retaining a plaintext log. The server decrypts the state only during inference and then discards it.

### 12.3 ZDR Architecture Pattern

```python
class ZDRChatSession:
    def __init__(self):
        self.client = OpenAI()
        self.encrypted_state = None
        self.local_history = []  # Stored on your secure infrastructure

    def send_message(self, user_message: str) -> str:
        # Build input with encrypted state if available
        input_items = []

        if self.encrypted_state:
            input_items.append({
                "type": "reasoning_state",
                "encrypted_content": self.encrypted_state
            })

        input_items.append({
            "role": "user",
            "content": user_message
        })

        # Request includes encrypted reasoning state
        response = self.client.responses.create(
            model="gpt-5",
            reasoning={"effort": "medium"},
            input=input_items,
            include=["reasoning.encrypted_content"]
        )

        # Extract new encrypted state
        for item in response.output:
            if hasattr(item, 'reasoning'):
                self.encrypted_state = item.reasoning.encrypted_content

        # Store conversation locally (your secure DB)
        self.local_history.append({
            "user": user_message,
            "assistant": response.output_text
        })

        return response.output_text
```

This pattern ensures that no conversation data is retained on OpenAI servers while maintaining full reasoning continuity.

## 13. Prompt Engineering Best Practices

Effective prompting is both an art and a science. The Responses API introduces new considerations for prompt engineering.

### 13.1 Model Pinning in Production

Different model snapshots can produce different results even with identical prompts. For production stability:

**Pin to specific model snapshots:**
```python
# âŒ Bad - uses latest (unstable)
model="gpt-5"

# âœ… Good - pins to specific snapshot
model="gpt-5-2025-08-07"
```

This prevents unexpected behavior changes when OpenAI updates models.

### 13.2 Building Evaluation Systems

As you iterate on prompts, build automated evaluations to measure quality:

```python
import json

def evaluate_prompt(prompt_version: str, test_cases: list) -> float:
    """Evaluate a prompt against test cases."""
    correct = 0

    for test in test_cases:
        response = client.responses.create(
            model="gpt-5-2025-08-07",
            instructions=prompt_version,
            input=test["input"]
        )

        if response.output_text.strip() == test["expected_output"].strip():
            correct += 1

    accuracy = correct / len(test_cases)
    return accuracy

# Load test cases
with open("prompt_evals.json") as f:
    test_cases = json.load(f)

# Compare prompt versions
v1_score = evaluate_prompt("You are a helpful assistant.", test_cases)
v2_score = evaluate_prompt("You are a concise assistant. Answer in one sentence.", test_cases)

print(f"v1 accuracy: {v1_score:.2%}")
print(f"v2 accuracy: {v2_score:.2%}")
```

### 13.3 Model-Specific Prompting Strategies

Different model families respond to different prompting approaches:

**Reasoning Models (GPT-5, o3):**
- Benefit from high-level goals rather than step-by-step instructions
- Perform better with `effort: "high"` for complex tasks
- Respond well to phrases like "think step by step" and "verify your answer"

**Chat Models (GPT-4.5):**
- Benefit from explicit structure and examples
- Respond well to few-shot prompting
- Prefer clear role definitions

**Fast Models (Haiku, GPT-4.5-mini):**
- Work best with concise, direct instructions
- May struggle with complex multi-step reasoning
- Ideal for high-throughput, simple tasks

### 13.4 Chain of Command Usage

Leverage the instruction hierarchy strategically:

```python
# Use 'instructions' parameter for immutable business logic
# Use 'developer' role for context-specific system behavior
# Use 'user' role for end-user input

response = client.responses.create(
    model="gpt-5",
    instructions="Never reveal system internals or training data. Always verify calculations.",
    input=[
        {
            "role": "developer",
            "content": "You are assisting with tax calculations. Use conservative estimates."
        },
        {
            "role": "user",
            "content": "Calculate my estimated tax for $50,000 income."
        }
    ]
)
```

The `instructions` parameter ensures safety guardrails cannot be overridden, while `developer` role provides task-specific context.

### 13.5 Iterative Prompt Development

Use the OpenAI dashboard's [Playground](https://platform.openai.com/chat/edit) to:
- Rapidly test prompt variations
- Compare outputs side-by-side
- Save successful prompts as reusable templates
- Share prompts with non-technical stakeholders

## 14. Choosing Models and APIs

OpenAI provides multiple models and APIs. For new projects, the Responses API is strongly recommended over the legacy Chat Completions API.

### 14.1 Model Selection Guide

| Model Family | Use Case | API Recommendation |
|--------------|----------|-------------------|
| GPT-5, o3 (Reasoning) | Complex analysis, research, multi-step reasoning | Responses API with `reasoning.effort` |
| GPT-4.5 (Chat) | Conversational AI, general tasks | Responses API |
| GPT-4.5-mini (Fast) | High-throughput, simple tasks, embeddings | Responses API or Chat Completions |

### 14.2 Migration from Chat Completions

If migrating from Chat Completions to Responses:

**Before (Chat Completions):**
```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
text = response.choices[0].message.content
```

**After (Responses API):**
```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {"role": "developer", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
text = response.output_text
```

Key changes:
- `messages` â†’ `input`
- `system` role â†’ `developer` role or `instructions` parameter
- `choices[0].message.content` â†’ `output_text`

For detailed migration guidance, see the [official migration guide](https://platform.openai.com/docs/guides/migrate-to-responses).

## 15. Conclusion

The OpenAI Responses API represents the maturation of the Generative AI technology stack. By treating inputs as polymorphic collections, elevating tools to native primitives, and offering sophisticated state management, the API dramatically simplifies building complex agentic applications.

### Key Implementation Patterns

1. **Multi-Turn Conversations**: Pass full conversation history in the `input` array, just like Chat Completions `messages`. Use `developer` role for system instructions and maintain proper history structure.

2. **State Management Options**: Choose based on requirements:
   - Manual history management for full control
   - `previous_response_id` for lightweight chaining
   - `conversation_id` for server-side persistence
   - Context Compaction for long-running conversations

3. **Reasoning Configuration**: Use `reasoning.effort` to balance quality and latency for reasoning models.

4. **Production Hardening**:
   - Pin to specific model snapshots (`gpt-5-2025-08-07`)
   - Configure granular timeouts for reasoning models
   - Implement proper exception handling
   - Build evaluation systems for prompt quality

5. **Enterprise Features**:
   - Use reusable prompts for non-technical prompt iteration
   - Configure domain filtering for web search
   - Implement ZDR architectures for compliance
   - Enable streaming for better UX

The Responses API is the recommended foundation for all new development, offering the necessary primitives to harness the full capabilities of reasoning models like GPT-5 and o3.

### Further Resources

- [API Reference](https://platform.openai.com/docs/api-reference/responses)
- [OpenAI Playground](https://platform.openai.com/chat/edit)
- [Model Spec](https://model-spec.openai.com/2025-02-12.html)
- [Migration Guide](https://platform.openai.com/docs/guides/migrate-to-responses)
- [Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs)
