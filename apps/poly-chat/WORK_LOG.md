## so far

i wrote what.md, talked about it with claude code (sonnet 4.5), added more info, generated how.md based on it, generated app code, generated test_ai to test ai-related code first, made/organized 7 accounts for ais, made __TEST_API_KEYS.json, test_ai passed, i adjusted test_keys, test_keys passed, i started testing the actual app (which barely worked with a lot of bugs when i quickly tested it after test_ai passed), reviewed providers code and found a lot of missing things such as timeout support, error handing, etc, generated a document based on what claude had learnt, generated code based on it to fix some of missing things.

it's been less than a week of vibe coding experience for me. i feel i still have much to learn. my new workflow is almost tolerable. yet, some things are definitely missing.

one factor that makes me think so is that i, within each day, dont even use 14% of the weekly limit of claude code pro subscription which is just 20 usd / month. people pay 5-10 times more for max plans. probably, i need to be even stupider. i am already stupid enough to generate everything and complain a lot, but i still try separating concerns (by at least asking ais all the time) and, while testing the app, review the code. i dont read it line by line, but i at least maintain vague understanding on how things are separated, which is responsible for what, etc to make sure there's no "tangled" code. i am starting to think, maybe, i should stop that as well.

for now, i still dont feel confident, using ai-generated code as-is. ais casually embed literals for tests to pass. they assume they are using the right packages. human beings are always "slightly" skeptical. we are never 100% sure. soon, there'll be ais that will be continually monitoring code like a capable human developer assigned to maintain a portion of code as their sole responsibility. until then, at least in my case, i feel i have a better chance of successful vibe coding if i, after or during tests, review what i am finalizing.

## internal documents

how to reduce context is one primarily important thing. i am working on this. i am still in the phase to get used to vibe coding and establish my own style (while of course reading a lot on how other people do it). i dont feel the seemingly-standard-ish "multiple instances of coding agents + zero preparation + microphone" way is for me. for small tools that are very different from what we have worked on before, it must one good option. however, there's almost always a "good practice" or two for what we are working on. in most cases, people have already solved it. there's cumulative knowledge. we only have to seek it. but then, to apply it to the project in front of us, we need to convert it into "context" that ais understand. this "context transition", which may not be proper working, is the very thing that's taking my time.

my initial attempt to tackle this was and still is updating what.md. at the end of development, what.md should know just enough for ais and future me to extract specific segments of specs. like, "how did i map a relative file path?" then the ai will definitely make a nice markdown document. i dont make such documents beforehand. part of me still wants to make 10-20 documents for segments of specs, implement each as a separate portion of code (for perfect SoC) and unite them so that each portion is highly modular and can be reused in other apps. but in python world, there's not much point in making 100 mini modules, each of which is like only 100 lines. so, i dont try making mini modules, i dont try making segmented spec documents and i only update what.md casually hoping ais can extract specs later WHEN I NEED THEM.

but as i am working with ais now, i am experiencing difficulties with ais assuming they are using the right packages and what they know are up-to-date. claude code generated code using google's old ai package (GoogleGenerativeAI). i asked it to migrate to the new one (GoogleGenAI). claude code did but didnt really update the actual code and started seeking hacks to extract things such as the endpoint url by accessing private things as things didnt work. claude code also wrote ai providers with no timeout support or error handling. so, the app once froze indefinitely. i need to deep-research on each ai's edge cases and document them first. only then, i can truly refine poly-chat's code.

then, what should i do with such documents? they will be super useful for other ai-related apps too. one great way to reduce context transition cost. i will call such documents "internal documents" from now on and store them in internal_docs folder. just "docs" is too vague. "resources" may sound like multi-type data that is used to render things, calculate things, etc. "helpers" sounds good to me, but some might think the folder must contain code of helper types. "prompts" wont suit this because a lot of the contents will be references. "internal_docs" is a little verbose, but everybody will understand: 1) it's not code, 2) it's not official or concrete.

for internal documents, i will use slug-ish file names. they are not source code files. they are not reserved names such as README.

then, i will rename _what.md, _how.md and _work_log.md back to versions without the prefix and store them in "internal_docs" from now on. what.md wont contain everything-everything and how.md wont be updated once it is generated based on a specific snapshot of what.md before implementation. work_log.md is not much more than what i am trying. work_log.md might be useful after some time if i ask ais to analyze ALL work_log.md files and establish my unified workflow based on what has actually worked. until then, it's just full of pathetic monologues. such files will fit better in "internal_docs". then, if i ever decide to release some tools publicly, it shouldnt be hard to write another mini tool to exclude the entire folder.
